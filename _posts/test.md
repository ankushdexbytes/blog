
# Spark Structured Streaming Checkpointing

In Structured Streaming, if you enable checkpointing for a streaming query, then you can restart the query after a failure and the restarted query will continue where the failed one left off, while ensuring fault tolerance and data consistency guarantees.

## Why needed?
The primary goal of checkpointing is to ensure the fault-tolerance of streaming jobs. Thanks to the metadata stored in checkpoint files you will be able to restart your processing in case of any failure - business logic or technical error.

Checkpoints are also important to guarantee at-least once processing in case of any failure in the middle of currently processed micro-batch.

## Enable checkpointing
To enable checkpointing, set the option `checkpointLocation` to a HDFS or cloud storage path. For example:
```scala
streamDataFrame.writeStream
  .format("csv")
  .option("path", "/outputStoragePath")
  .option("checkpointLocation", "/checkpointPath")
  .start()
```
## What is stored in Checkpoint Path
Checkpoint is a physical directory and responsible for storing 4 types of directories:

-   **source :** files in this directory contain the information about different sources used in the streaming query.
-   **offsets :** The Offset directory contains a file with information about data that will be processed in given micro-batch execution. It's generated before the physical execution of the micro-batch.
-   **commit logs :** The Commit logs directory is a marker kind of file and generated for each micro-batch. 
-   **state :** it'll be responsible for the storage of state generated by stateful processing logic.

## How checkpoint enforces at least delivery


<!--stackedit_data:
eyJoaXN0b3J5IjpbODU4NjIwNDY0LDc4NzEyNzI1MSwtMTg0Nz
Y5NjM3NywtMTY5MzEzODM1MSwxNjU2MTMyNjI4LDI0MTczODQ3
Nyw2ODQyMDUzNzAsMTYwMDQwMzQzMSwtNzI3MDE1MDA3LC05NT
kxMzkyNzgsOTg1NjM1NjU0LC0xNTQyNjA4MjU0LC0xOTQyMjgz
MjIwLC00MjIzMTg5OTQsLTMyNDI4MDczMCwtMjExNDUwMDQ4My
wtMjEyMjQ2NTc4MSw0NTg4OTAwMTMsLTE2NTY4NzcwMTAsMTE4
MzQ1MjM0OF19
-->