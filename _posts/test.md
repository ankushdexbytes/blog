
# Spark Structured Streaming Checkpointing

In Structured Streaming, if you enable checkpointing for a streaming query, then you can restart the query after a failure and the restarted query will continue where the failed one left off, while ensuring fault tolerance and data consistency guarantees.

## Why needed?
The primary goal of checkpointing is to ensure the fault-tolerance of streaming jobs. Thanks to the metadata stored in checkpoint files you will be able to restart your processing in case of any failure - business logic or technical error.

Checkpoints are also important to guarantee at-least once processing in case of any failure in the middle of currently processed micro-batch.

## Enable checkpointing
To enable checkpointing, set the option `checkpointLocation` to a HDFS or cloud storage path. For example:
```scala
streamDataFrame.writeStream
  .format("csv")
  .option("path", "/outputStoragePath")
  .option("checkpointLocation", "/checkpointPath")
  .start()
```
## What is stored in Checkpoint Path
Checkpoint is a physical directory and responsible for storing 4 types of directories:

-   **source :** files in this directory contain the information about different sources used in the streaming query.
-   **offsets :** The Offset directory contains a file with information about data that will be processed in given micro-batch execution. It's generated before the physical execution of the micro-batch.
-   **commits :** The Commits directory is a marker kind of file and generated for each micro-batch. 
-   **state :** it'll be responsible for the storage of state generated by stateful processing logic.

## How checkpoint enforces exactly once delivery guarantee
Let's see how checkpoint works in File Sink.
```scala
streamDataFrame.writeStream
  .format("csv")
  .option("path", "/outputStoragePath")
  .option("checkpointLocation", "/checkpointPath")
  .start()
```
In the above example we have provided **Checkpoint directory** and **Storage directory** Paths.

 - **Under the Checkpoint directory :** we have an offsets directory and that commits directory.
 - **Under the Storage directory :** we have an actual data files and _spark_metadata directory.

### what happens when we start processing :

**First Step :**  Creates a file for Micro Batch One under Offsets directory. It contains offsets against which spark extract from the source.
**Second Step :**  Next step it will extract the data from source and store the  output files under Storage directory. The number of output files are depends on the number of partitions.
**Third Step :**  Next step it will create file under _spark_metadata directory and store file names which have been written for Micro Batch one.
**Fourth Step :** Finally it will create a file under Commits folder once Micro Batch one is committed.

> **These Steps will executed for each Micro Batch run.**

### what happens in case of job failure:
suppose job starts processing again for Micro Batch two, a file is created under offsets, and it then starts to write the data to storage directory. Assume one file is written, and there is a job failure. This means partial output has been written out.



<!--stackedit_data:
eyJoaXN0b3J5IjpbLTI5OTY2MTI2OSwtMTUyMjM0MTI4NywtND
c0NDY3MTIxLDg1ODYyMDQ2NCw3ODcxMjcyNTEsLTE4NDc2OTYz
NzcsLTE2OTMxMzgzNTEsMTY1NjEzMjYyOCwyNDE3Mzg0NzcsNj
g0MjA1MzcwLDE2MDA0MDM0MzEsLTcyNzAxNTAwNywtOTU5MTM5
Mjc4LDk4NTYzNTY1NCwtMTU0MjYwODI1NCwtMTk0MjI4MzIyMC
wtNDIyMzE4OTk0LC0zMjQyODA3MzAsLTIxMTQ1MDA0ODMsLTIx
MjI0NjU3ODFdfQ==
-->